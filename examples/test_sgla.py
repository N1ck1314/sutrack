"""
æµ‹è¯•è„šæœ¬ - SGLAæ¨¡å—
æµ‹è¯• Similarity-Guided Layer-Adaptive (SGLA) çš„æ ¸å¿ƒç»„ä»¶
"""
import sys
import os
sys.path.append(os.getcwd())

import torch
import torch.nn as nn
from lib.models.sutrack_SGLA.sgla_modules import SelectionModule, SimilarityLoss, LayerAdaptiveWrapper

def test_sgla_modules():
    print("=" * 60)
    print("æµ‹è¯• 1: SelectionModule")
    print("=" * 60)
    
    B, N, C = 2, 196, 384
    num_layers = 12
    x = torch.randn(B, N, C)
    
    selector = SelectionModule(C, num_layers)
    probs = selector(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºæ¦‚ç‡å½¢çŠ¶: {probs.shape}")
    print(f"æ¦‚ç‡èŒƒå›´: [{probs.min().item():.4f}, {probs.max().item():.4f}]")
    
    assert probs.shape == (B, num_layers)
    assert (probs >= 0).all() and (probs <= 1).all()
    print("âœ… SelectionModule æµ‹è¯•é€šè¿‡")

    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: SimilarityLoss")
    print("=" * 60)
    
    loss_fn = SimilarityLoss(mode='cosine')
    
    # æ¨¡æ‹Ÿç›¸ä¼¼ç‰¹å¾
    f1 = torch.randn(B, N, C)
    f2 = f1 + torch.randn(B, N, C) * 0.1
    features_similar = [f1, f2]
    loss_similar = loss_fn(features_similar)
    
    # æ¨¡æ‹Ÿä¸ç›¸ä¼¼ç‰¹å¾
    f3 = torch.randn(B, N, C)
    features_dissimilar = [f1, f3]
    loss_dissimilar = loss_fn(features_dissimilar)
    
    print(f"ç›¸ä¼¼ç‰¹å¾ Loss: {loss_similar.item():.4f}")
    print(f"ä¸ç›¸ä¼¼ç‰¹å¾ Loss: {loss_dissimilar.item():.4f}")
    
    # ç›¸ä¼¼ç‰¹å¾çš„ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±åº”è¯¥æ›´é«˜ (è¶‹è¿‘äº1)
    # å› ä¸º SimilarityLoss è®¡ç®—çš„æ˜¯å‡å€¼ç›¸ä¼¼åº¦ï¼Œè€Œä¸æ˜¯ 1-sim
    # SGLATrack è®ºæ–‡ä¸­ï¼Œç›¸ä¼¼åº¦è¶Šé«˜è¡¨ç¤ºå†—ä½™è¶Šå¤§ï¼Œé€šå¸¸å¸Œæœ›æœ€å°åŒ–è¿™ä¸ªæŸå¤±æˆ–è€…ç”¨äºæŒ‡å¯¼
    assert loss_similar > loss_dissimilar
    print("âœ… SimilarityLoss æµ‹è¯•é€šè¿‡")

    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: LayerAdaptiveWrapper")
    print("=" * 60)
    
    class MockBlock(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(C, C)
        def forward(self, x, **kwargs):
            return x + self.linear(x)
            
    block = MockBlock()
    wrapper = LayerAdaptiveWrapper(block)
    
    # æµ‹è¯•æ¨ç†æ¨¡å¼ (prob > 0.5)
    wrapper.eval()
    prob_on = torch.tensor([0.9])
    out_on = wrapper(x, prob=prob_on)
    print("æ¨ç†æ¨¡å¼ (prob=0.9): Block å·²æ‰§è¡Œ")
    assert not torch.allclose(x, out_on)
    
    # æµ‹è¯•æ¨ç†æ¨¡å¼ (prob < 0.5)
    prob_off = torch.tensor([0.1])
    out_off = wrapper(x, prob=prob_off)
    print("æ¨ç†æ¨¡å¼ (prob=0.1): Block å·²è·³è¿‡")
    assert torch.allclose(x, out_off)
    
    # æµ‹è¯•è®­ç»ƒæ¨¡å¼ (éšæœºæ€§)
    wrapper.train()
    print("è®­ç»ƒæ¨¡å¼: å¯ç”¨éšæœºé‡‡æ ·")
    out_train = wrapper(x, prob=torch.tensor([0.5]))
    print(f"è®­ç»ƒæ¨¡å¼è¾“å‡ºå½¢çŠ¶: {out_train.shape}")
    assert out_train.shape == x.shape
    
    print("âœ… LayerAdaptiveWrapper æµ‹è¯•é€šè¿‡")

if __name__ == '__main__':
    test_sgla_modules()
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰ SGLA æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)
